# OpsLens â€” Multimodal On-Call Copilot

<div align="center">

**An AI-powered incident response system that transforms chaos into clarity**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14.0-black.svg)](https://nextjs.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-blue.svg)](https://www.postgresql.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

</div>

## ğŸ¯ Overview

OpsLens is an end-to-end incident-response copilot that transforms messy, real-world signals (alerts, logs, metrics, traces, screenshots, runbooks, PRs) into actionable intelligence:

- **Live incident timeline** - "What changed, when, and likely why"
- **Ranked hypotheses** - AI-generated root cause analysis with supporting evidence
- **Actionable next steps** - Queries to run, owners to page, rollback plans
- **Postmortem drafts** - Automatic generation with root-cause candidates and follow-ups

### The Problem It Solves

On-call engineers waste precious time context-switching across tools (PagerDuty â†’ Slack â†’ Datadog/Grafana â†’ GitHub â†’ runbooks). OpsLens reduces MTTR (Mean Time To Resolution) by:

- **Connecting evidence across systems** - Automatically correlates alerts, deployments, and metrics
- **Multimodal understanding** - Uses Vision-Language Models (VLM) to analyze dashboard screenshots
- **Structured incident state** - Maintains a deterministic, trustworthy incident timeline
- **AI-powered insights** - Generates hypotheses and suggests actions based on historical patterns

## âœ¨ Features

### Core Functionality

- **ğŸ”— Real-time Integrations**
  - GitHub - Fetches recent PR merges and deployment information
  - PagerDuty - Pulls active incidents and alerts
  - Extensible architecture for additional integrations (Slack, Datadog, Grafana, etc.)

- **ğŸ¤– AI-Powered Analysis**
  - **LLM Integration** (Llama 3.1) - Summarizes logs, generates hypotheses, drafts postmortems
  - **VLM Integration** (Qwen2.5-VL) - Analyzes dashboard screenshots and extracts insights
  - **RAG System** (BGE-M3) - Semantic search over runbooks and historical postmortems

- **ğŸ“Š Incident Management**
  - Event-driven timeline generation
  - Deterministic incident state machine
  - Evidence correlation and ranking
  - Action tracking and completion

- **ğŸ¨ Modern UI**
  - War Room view with live timeline
  - Evidence viewer with screenshot understanding
  - Hypothesis ranking with confidence scores
  - Action queue with completion tracking

## ğŸ—ï¸ Architecture

### Tech Stack

**Backend:**
- **FastAPI** - High-performance async API framework
- **PostgreSQL + pgvector** - Relational database with vector embeddings support
- **Celery + Redis** - Distributed task queue for async processing
- **Hugging Face Inference API** - LLM, VLM, and embedding models

**Frontend:**
- **Next.js 14** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling

**Infrastructure:**
- **Docker Compose** - Containerized development environment
- **Postgres with pgvector extension** - Vector similarity search
- **Redis** - Task queue and caching

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚  PostgreSQL â”‚
â”‚  (Next.js)  â”‚     â”‚   Backend    â”‚     â”‚  + pgvector â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€â”€â”€â”€â–¶ Celery Workers
                            â”‚         â”‚
                            â”‚         â”œâ”€â”€ GitHub Integration
                            â”‚         â”œâ”€â”€ PagerDuty Integration
                            â”‚         â””â”€â”€ ML Processing
                            â”‚
                            â””â”€â”€â”€â”€â–¶ Hugging Face API
                                   (LLM, VLM, Embeddings)
```

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop installed and running
- API keys for:
  - [Hugging Face](https://huggingface.co/settings/tokens) (free tier available)
  - [GitHub](https://github.com/settings/tokens) (Personal Access Token with `repo` scope)
  - [PagerDuty](https://support.pagerduty.com/docs/api-keys) (API key from your account)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/OpsLens.git
   cd OpsLens
   ```

2. **Set up environment variables:**
   ```bash
   cp secrets.env.example secrets.env
   # Edit secrets.env and add your API keys
   ```

3. **Start the application:**
   ```bash
   ./setup.sh
   ```
   
   Or manually:
   ```bash
   docker-compose up -d
   docker-compose exec backend python -m app.db.init_db
   docker-compose exec backend python -m app.data.generate_synthetic
   ```

4. **Access the application:**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs

## ğŸ“ Project Structure

```
OpsLens/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # REST API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ incidents.py
â”‚   â”‚   â”‚   â”œâ”€â”€ evidence.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hypotheses.py
â”‚   â”‚   â”‚   â”œâ”€â”€ runbooks.py
â”‚   â”‚   â”‚   â””â”€â”€ integrations.py
â”‚   â”‚   â”œâ”€â”€ db/             # Database models and setup
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â””â”€â”€ init_db.py
â”‚   â”‚   â”œâ”€â”€ integrations/   # External service integrations
â”‚   â”‚   â”‚   â”œâ”€â”€ github.py
â”‚   â”‚   â”‚   â””â”€â”€ pagerduty.py
â”‚   â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ml_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â”‚   â””â”€â”€ incident_service.py
â”‚   â”‚   â”œâ”€â”€ workers/         # Celery async tasks
â”‚   â”‚   â”‚   â”œâ”€â”€ incident_worker.py
â”‚   â”‚   â”‚   â””â”€â”€ evidence_worker.py
â”‚   â”‚   â”œâ”€â”€ data/            # Data generation utilities
â”‚   â”‚   â”‚   â””â”€â”€ generate_synthetic.py
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI application
â”‚   â”‚   â””â”€â”€ celery_app.py   # Celery configuration
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/                # Next.js frontend
â”‚   â”œâ”€â”€ app/                # Next.js app directory
â”‚   â”‚   â”œâ”€â”€ page.tsx         # Incident list
â”‚   â”‚   â”œâ”€â”€ incidents/[id]/  # Incident detail page
â”‚   â”‚   â””â”€â”€ layout.tsx
â”‚   â”œâ”€â”€ lib/                 # Utilities
â”‚   â”‚   â”œâ”€â”€ api.ts          # API client
â”‚   â”‚   â””â”€â”€ types.ts        # TypeScript types
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml       # Docker orchestration
â”œâ”€â”€ setup.sh                 # Setup script
â”œâ”€â”€ secrets.env.example      # Environment template
â””â”€â”€ README.md
```

## ğŸ”§ Development

### Running Locally

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f [service_name]

# Restart a service
docker-compose restart [service_name]

# Stop all services
docker-compose down
```

### Testing Integrations

Test your GitHub and PagerDuty integrations:

```bash
# Test GitHub
curl http://localhost:8000/api/v1/integrations/test/github

# Test PagerDuty
curl http://localhost:8000/api/v1/integrations/test/pagerduty

# Test all
curl http://localhost:8000/api/v1/integrations/test/all
```

### Adding New Integrations

1. Create integration module in `backend/app/integrations/`
2. Add API endpoints in `backend/app/api/integrations.py`
3. Create Celery tasks for async processing
4. Update incident worker to use new integration

## ğŸ“– Usage

### Creating an Incident

1. Navigate to http://localhost:3000
2. View existing incidents or create a new one via API
3. Click on an incident to view details

### Generating Timeline

1. Open an incident detail page
2. Click "Generate Timeline" button
3. System will:
   - Fetch recent GitHub PR merges (last 24 hours)
   - Fetch recent PagerDuty incidents
   - Correlate events chronologically
   - Display in timeline view

### Analyzing Screenshots

1. Go to the Evidence tab
2. Upload a dashboard screenshot
3. VLM will analyze and extract:
   - Error patterns
   - Metric anomalies
   - Important insights

### Generating Hypotheses

1. Click "Generate Hypotheses" on an incident
2. AI will analyze evidence and generate:
   - Root cause hypotheses
   - Confidence scores
   - Supporting evidence links

## ğŸ§ª Testing

The project includes realistic synthetic data for testing:

```bash
docker-compose exec backend python -m app.data.generate_synthetic
```

This generates 5 realistic incident scenarios with:
- Coherent timelines
- Relevant evidence
- Realistic hypotheses
- Actionable next steps

## ğŸ” Security

- API keys stored in `secrets.env` (not committed to git)
- Environment-based configuration
- Input validation on all endpoints
- SQL injection protection via SQLAlchemy ORM

## ğŸ›£ï¸ Roadmap

- [ ] Slack integration for incident threads
- [ ] Datadog/Grafana metrics integration
- [ ] OpenTelemetry trace correlation
- [ ] Kubernetes event ingestion
- [ ] Webhook support for real-time updates
- [ ] Advanced RAG with fine-tuning
- [ ] Evaluation harness for ML models
- [ ] Role-based access control
- [ ] Audit logging
- [ ] Multi-tenant support

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) for ML model hosting
- [FastAPI](https://fastapi.tiangolo.com/) for the excellent framework
- [Next.js](https://nextjs.org/) for the React framework
- [pgvector](https://github.com/pgvector/pgvector) for vector similarity search

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ for on-call engineers everywhere**
